\hypertarget{namespacem__mpi__proxy}{}\section{m\+\_\+mpi\+\_\+proxy Module Reference}
\label{namespacem__mpi__proxy}\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}


This module serves as a proxy to the parameters and subroutines available in the M\+PI implementation\textquotesingle{}s M\+PI module. Specifically, the role of the proxy is to harness basic M\+PI commands into more complex procedures as to achieve the required communication goals for the post-\/process.  


\subsection*{Functions/\+Subroutines}
\begin{DoxyCompactItemize}
\item 
subroutine \hyperlink{namespacem__mpi__proxy_a9bc4c617505152d3cc553e5bc25c1ee1}{s\+\_\+mpi\+\_\+initialize} ()
\begin{DoxyCompactList}\small\item\em The subroutine intializes the M\+PI environment and queries both the number of processors that will be available for the job as well as the local processor rank. \end{DoxyCompactList}\item 
subroutine \hyperlink{namespacem__mpi__proxy_a04ac565bad2b22dc045a5eeb4f516e2e}{s\+\_\+mpi\+\_\+abort} ()
\begin{DoxyCompactList}\small\item\em The subroutine terminates the M\+PI execution environment. \end{DoxyCompactList}\end{DoxyCompactItemize}
\begin{Indent}\textbf{ q\+\_\+cons\+\_\+vf Conservative variables}\par
{\em This subroutine defines local and global sizes for the data }\begin{DoxyCompactItemize}
\item 
subroutine \hyperlink{namespacem__mpi__proxy_a2ff35ede51e90c483969e44c31303415}{s\+\_\+initialize\+\_\+mpi\+\_\+data} (q\+\_\+cons\+\_\+vf)
\item 
subroutine \hyperlink{namespacem__mpi__proxy_abfbc42cea69273bc9fa4a2d78f636eb1}{s\+\_\+mpi\+\_\+barrier} ()
\begin{DoxyCompactList}\small\item\em Halts all processes until all have reached barrier. \end{DoxyCompactList}\item 
subroutine \hyperlink{namespacem__mpi__proxy_a015ee2c0892e9cfcb858da8f27b646d5}{s\+\_\+initialize\+\_\+mpi\+\_\+proxy\+\_\+module} ()
\begin{DoxyCompactList}\small\item\em Computation of parameters, allocation procedures, and/or any other tasks needed to properly setup the module. \end{DoxyCompactList}\item 
subroutine \hyperlink{namespacem__mpi__proxy_a69660c5fe9302a8c0496b622fa3b5286}{s\+\_\+mpi\+\_\+bcast\+\_\+user\+\_\+inputs} ()
\begin{DoxyCompactList}\small\item\em Since only processor with rank 0 is in charge of reading and checking the consistency of the user provided inputs, these are not available to the remaining processors. This subroutine is then in charge of broadcasting the required information. \end{DoxyCompactList}\item 
subroutine \hyperlink{namespacem__mpi__proxy_a80c5e235786545276fe6ffa06965017f}{s\+\_\+mpi\+\_\+decompose\+\_\+computational\+\_\+domain} ()
\begin{DoxyCompactList}\small\item\em This subroutine takes care of efficiently distributing the computational domain among the available processors as well as recomputing some of the global parameters so that they reflect the configuration of sub-\/domain that is overseen by the local processor. \end{DoxyCompactList}\item 
subroutine \hyperlink{namespacem__mpi__proxy_a8e48d59a04981a6594f25ad7a7562492}{s\+\_\+mpi\+\_\+sendrecv\+\_\+grid\+\_\+vars\+\_\+buffer\+\_\+regions} (pbc\+\_\+loc, sweep\+\_\+coord)
\begin{DoxyCompactList}\small\item\em Communicates the buffer regions associated with the grid variables with processors in charge of the neighbooring sub-\/domains. Note that only cell-\/width spacings feature buffer regions so that no information relating to the cell-\/boundary locations is communicated. \end{DoxyCompactList}\item 
subroutine \hyperlink{namespacem__mpi__proxy_a4d7ec0d1976967504babdf44ec83c1b1}{s\+\_\+mpi\+\_\+sendrecv\+\_\+cons\+\_\+vars\+\_\+buffer\+\_\+regions} (q\+\_\+cons\+\_\+vf, pbc\+\_\+loc, sweep\+\_\+coord)
\begin{DoxyCompactList}\small\item\em Communicates buffer regions associated with conservative variables with processors in charge of the neighbooring sub-\/domains. \end{DoxyCompactList}\item 
subroutine \hyperlink{namespacem__mpi__proxy_adaa028bb99f844487de8d6e4507e7734}{s\+\_\+mpi\+\_\+reduce\+\_\+maxloc} (var\+\_\+loc)
\begin{DoxyCompactList}\small\item\em The following subroutine takes the first element of the 2-\/element inputted variable and determines its maximum value on the entire computational domain. The result is stored back into the first element of the variable while the rank of the processor that is in charge of the sub-\/ domain containing the maximum is stored into the second element of the variable. \end{DoxyCompactList}\item 
subroutine \hyperlink{namespacem__mpi__proxy_ab2ced8f095b812fcc355539c2c5fa162}{s\+\_\+mpi\+\_\+gather\+\_\+spatial\+\_\+extents} (spatial\+\_\+extents)
\begin{DoxyCompactList}\small\item\em This subroutine gathers the Silo database metadata for the spatial extents in order to boost the performance of the multidimensional visualization. \end{DoxyCompactList}\item 
subroutine \hyperlink{namespacem__mpi__proxy_ac15a15ba12e6110bb015af0ece5bba47}{s\+\_\+mpi\+\_\+defragment\+\_\+1d\+\_\+grid\+\_\+variable} ()
\begin{DoxyCompactList}\small\item\em This subroutine collects the sub-\/domain cell-\/boundary or cell-\/center locations data from all of the processors and puts back together the grid of the entire computational domain on the rank 0 processor. This is only done for 1D simulations. \end{DoxyCompactList}\item 
subroutine \hyperlink{namespacem__mpi__proxy_a0b226acd5a9097566685604292c3ae0d}{s\+\_\+mpi\+\_\+gather\+\_\+data\+\_\+extents} (q\+\_\+sf, data\+\_\+extents)
\begin{DoxyCompactList}\small\item\em This subroutine gathers the Silo database metadata for the flow variable\textquotesingle{}s extents as to boost performance of the multidimensional visualization. \end{DoxyCompactList}\item 
subroutine \hyperlink{namespacem__mpi__proxy_ae5286531f6390643aad1e4db5d5f6d91}{s\+\_\+mpi\+\_\+defragment\+\_\+1d\+\_\+flow\+\_\+variable} (q\+\_\+sf, q\+\_\+root\+\_\+sf)
\begin{DoxyCompactList}\small\item\em This subroutine gathers the sub-\/domain flow variable data from all of the processors and puts it back together for the entire computational domain on the rank 0 processor. This is only done for 1D simulations. \end{DoxyCompactList}\item 
subroutine \hyperlink{namespacem__mpi__proxy_ac984c84fe4140876d6600250af9807da}{s\+\_\+finalize\+\_\+mpi\+\_\+proxy\+\_\+module} ()
\begin{DoxyCompactList}\small\item\em Deallocation procedures for the module. \end{DoxyCompactList}\item 
subroutine \hyperlink{namespacem__mpi__proxy_a43fbda10c02ec8bc1fc572c83090f2e5}{s\+\_\+mpi\+\_\+finalize} ()
\begin{DoxyCompactList}\small\item\em Finalization of all M\+PI related processes. \end{DoxyCompactList}\end{DoxyCompactItemize}
\end{Indent}
\subsection*{Variables}
\begin{Indent}\textbf{ Buffers of the conservative variables recieved/sent from/to neighbooring}\par
{\em processors. Note that these variables are structured as vectors rather than arrays. }\begin{DoxyCompactItemize}
\item 
real(kind(0d0)), dimension(\+:), allocatable \hyperlink{namespacem__mpi__proxy_af08f3246d9efac15d6391dc4205d4611}{q\+\_\+cons\+\_\+buffer\+\_\+in}
\item 
real(kind(0d0)), dimension(\+:), allocatable \hyperlink{namespacem__mpi__proxy_a3d449655a88a9e5248af35a82caf877a}{q\+\_\+cons\+\_\+buffer\+\_\+out}
\end{DoxyCompactItemize}
\end{Indent}
\begin{Indent}\textbf{ Recieve counts and displacement vector variables, respectively, used in}\par
{\em enabling M\+PI to gather varying amounts of data from all processes to the root process }\begin{DoxyCompactItemize}
\item 
integer, dimension(\+:), allocatable \hyperlink{namespacem__mpi__proxy_a2198e825f0884d4ee9e96b6efdb69cee}{recvcounts}
\item 
integer, dimension(\+:), allocatable \hyperlink{namespacem__mpi__proxy_aebaa6e3cc66d2431c5fb49896d40d7e6}{displs}
\end{DoxyCompactItemize}
\end{Indent}
\begin{Indent}\textbf{ Generic flags used to identify and report M\+PI errors}\par
\begin{DoxyCompactItemize}
\item 
integer, private \hyperlink{namespacem__mpi__proxy_ae5709407e3600d19d79b183e409bb982}{err\+\_\+code}
\item 
integer, private \hyperlink{namespacem__mpi__proxy_a306ba163b09cfc692125f2c0ba82ef8c}{ierr}
\end{DoxyCompactItemize}
\end{Indent}


\subsection{Detailed Description}
This module serves as a proxy to the parameters and subroutines available in the M\+PI implementation\textquotesingle{}s M\+PI module. Specifically, the role of the proxy is to harness basic M\+PI commands into more complex procedures as to achieve the required communication goals for the post-\/process. 

\subsection{Function/\+Subroutine Documentation}
\mbox{\Hypertarget{namespacem__mpi__proxy_ac984c84fe4140876d6600250af9807da}\label{namespacem__mpi__proxy_ac984c84fe4140876d6600250af9807da}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!s\+\_\+finalize\+\_\+mpi\+\_\+proxy\+\_\+module@{s\+\_\+finalize\+\_\+mpi\+\_\+proxy\+\_\+module}}
\index{s\+\_\+finalize\+\_\+mpi\+\_\+proxy\+\_\+module@{s\+\_\+finalize\+\_\+mpi\+\_\+proxy\+\_\+module}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{s\+\_\+finalize\+\_\+mpi\+\_\+proxy\+\_\+module()}{s\_finalize\_mpi\_proxy\_module()}}
{\footnotesize\ttfamily subroutine m\+\_\+mpi\+\_\+proxy\+::s\+\_\+finalize\+\_\+mpi\+\_\+proxy\+\_\+module (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Deallocation procedures for the module. 



Definition at line 1871 of file m\+\_\+mpi\+\_\+proxy.\+f90.

\mbox{\Hypertarget{namespacem__mpi__proxy_a2ff35ede51e90c483969e44c31303415}\label{namespacem__mpi__proxy_a2ff35ede51e90c483969e44c31303415}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!s\+\_\+initialize\+\_\+mpi\+\_\+data@{s\+\_\+initialize\+\_\+mpi\+\_\+data}}
\index{s\+\_\+initialize\+\_\+mpi\+\_\+data@{s\+\_\+initialize\+\_\+mpi\+\_\+data}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{s\+\_\+initialize\+\_\+mpi\+\_\+data()}{s\_initialize\_mpi\_data()}}
{\footnotesize\ttfamily subroutine m\+\_\+mpi\+\_\+proxy\+::s\+\_\+initialize\+\_\+mpi\+\_\+data (\begin{DoxyParamCaption}\item[{type(\hyperlink{structm__derived__types_1_1scalar__field}{scalar\+\_\+field}), dimension(sys\+\_\+size), intent(in)}]{q\+\_\+cons\+\_\+vf }\end{DoxyParamCaption})}



Definition at line 125 of file m\+\_\+mpi\+\_\+proxy.\+f90.

\mbox{\Hypertarget{namespacem__mpi__proxy_a015ee2c0892e9cfcb858da8f27b646d5}\label{namespacem__mpi__proxy_a015ee2c0892e9cfcb858da8f27b646d5}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!s\+\_\+initialize\+\_\+mpi\+\_\+proxy\+\_\+module@{s\+\_\+initialize\+\_\+mpi\+\_\+proxy\+\_\+module}}
\index{s\+\_\+initialize\+\_\+mpi\+\_\+proxy\+\_\+module@{s\+\_\+initialize\+\_\+mpi\+\_\+proxy\+\_\+module}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{s\+\_\+initialize\+\_\+mpi\+\_\+proxy\+\_\+module()}{s\_initialize\_mpi\_proxy\_module()}}
{\footnotesize\ttfamily subroutine m\+\_\+mpi\+\_\+proxy\+::s\+\_\+initialize\+\_\+mpi\+\_\+proxy\+\_\+module (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Computation of parameters, allocation procedures, and/or any other tasks needed to properly setup the module. 



Definition at line 175 of file m\+\_\+mpi\+\_\+proxy.\+f90.

\mbox{\Hypertarget{namespacem__mpi__proxy_a04ac565bad2b22dc045a5eeb4f516e2e}\label{namespacem__mpi__proxy_a04ac565bad2b22dc045a5eeb4f516e2e}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!s\+\_\+mpi\+\_\+abort@{s\+\_\+mpi\+\_\+abort}}
\index{s\+\_\+mpi\+\_\+abort@{s\+\_\+mpi\+\_\+abort}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{s\+\_\+mpi\+\_\+abort()}{s\_mpi\_abort()}}
{\footnotesize\ttfamily subroutine m\+\_\+mpi\+\_\+proxy\+::s\+\_\+mpi\+\_\+abort (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



The subroutine terminates the M\+PI execution environment. 



Definition at line 113 of file m\+\_\+mpi\+\_\+proxy.\+f90.

\mbox{\Hypertarget{namespacem__mpi__proxy_abfbc42cea69273bc9fa4a2d78f636eb1}\label{namespacem__mpi__proxy_abfbc42cea69273bc9fa4a2d78f636eb1}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!s\+\_\+mpi\+\_\+barrier@{s\+\_\+mpi\+\_\+barrier}}
\index{s\+\_\+mpi\+\_\+barrier@{s\+\_\+mpi\+\_\+barrier}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{s\+\_\+mpi\+\_\+barrier()}{s\_mpi\_barrier()}}
{\footnotesize\ttfamily subroutine m\+\_\+mpi\+\_\+proxy\+::s\+\_\+mpi\+\_\+barrier (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Halts all processes until all have reached barrier. 



Definition at line 164 of file m\+\_\+mpi\+\_\+proxy.\+f90.

\mbox{\Hypertarget{namespacem__mpi__proxy_a69660c5fe9302a8c0496b622fa3b5286}\label{namespacem__mpi__proxy_a69660c5fe9302a8c0496b622fa3b5286}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!s\+\_\+mpi\+\_\+bcast\+\_\+user\+\_\+inputs@{s\+\_\+mpi\+\_\+bcast\+\_\+user\+\_\+inputs}}
\index{s\+\_\+mpi\+\_\+bcast\+\_\+user\+\_\+inputs@{s\+\_\+mpi\+\_\+bcast\+\_\+user\+\_\+inputs}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{s\+\_\+mpi\+\_\+bcast\+\_\+user\+\_\+inputs()}{s\_mpi\_bcast\_user\_inputs()}}
{\footnotesize\ttfamily subroutine m\+\_\+mpi\+\_\+proxy\+::s\+\_\+mpi\+\_\+bcast\+\_\+user\+\_\+inputs (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Since only processor with rank 0 is in charge of reading and checking the consistency of the user provided inputs, these are not available to the remaining processors. This subroutine is then in charge of broadcasting the required information. 



Definition at line 273 of file m\+\_\+mpi\+\_\+proxy.\+f90.

\mbox{\Hypertarget{namespacem__mpi__proxy_a80c5e235786545276fe6ffa06965017f}\label{namespacem__mpi__proxy_a80c5e235786545276fe6ffa06965017f}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!s\+\_\+mpi\+\_\+decompose\+\_\+computational\+\_\+domain@{s\+\_\+mpi\+\_\+decompose\+\_\+computational\+\_\+domain}}
\index{s\+\_\+mpi\+\_\+decompose\+\_\+computational\+\_\+domain@{s\+\_\+mpi\+\_\+decompose\+\_\+computational\+\_\+domain}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{s\+\_\+mpi\+\_\+decompose\+\_\+computational\+\_\+domain()}{s\_mpi\_decompose\_computational\_domain()}}
{\footnotesize\ttfamily subroutine m\+\_\+mpi\+\_\+proxy\+::s\+\_\+mpi\+\_\+decompose\+\_\+computational\+\_\+domain (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



This subroutine takes care of efficiently distributing the computational domain among the available processors as well as recomputing some of the global parameters so that they reflect the configuration of sub-\/domain that is overseen by the local processor. 



Definition at line 440 of file m\+\_\+mpi\+\_\+proxy.\+f90.

\mbox{\Hypertarget{namespacem__mpi__proxy_ae5286531f6390643aad1e4db5d5f6d91}\label{namespacem__mpi__proxy_ae5286531f6390643aad1e4db5d5f6d91}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!s\+\_\+mpi\+\_\+defragment\+\_\+1d\+\_\+flow\+\_\+variable@{s\+\_\+mpi\+\_\+defragment\+\_\+1d\+\_\+flow\+\_\+variable}}
\index{s\+\_\+mpi\+\_\+defragment\+\_\+1d\+\_\+flow\+\_\+variable@{s\+\_\+mpi\+\_\+defragment\+\_\+1d\+\_\+flow\+\_\+variable}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{s\+\_\+mpi\+\_\+defragment\+\_\+1d\+\_\+flow\+\_\+variable()}{s\_mpi\_defragment\_1d\_flow\_variable()}}
{\footnotesize\ttfamily subroutine m\+\_\+mpi\+\_\+proxy\+::s\+\_\+mpi\+\_\+defragment\+\_\+1d\+\_\+flow\+\_\+variable (\begin{DoxyParamCaption}\item[{real(kind(0d0)), dimension(0\+:m,0\+:0,0\+:0), intent(in)}]{q\+\_\+sf,  }\item[{real(kind(0d0)), dimension(0\+:m\+\_\+root,0\+:0,0\+:0), intent(inout)}]{q\+\_\+root\+\_\+sf }\end{DoxyParamCaption})}



This subroutine gathers the sub-\/domain flow variable data from all of the processors and puts it back together for the entire computational domain on the rank 0 processor. This is only done for 1D simulations. 


\begin{DoxyParams}{Parameters}
{\em q\+\_\+sf} & Flow variable defined on a single computational sub-\/domain \\
\hline
{\em q\+\_\+root\+\_\+sf} & Flow variable defined on the entire computational domain \\
\hline
\end{DoxyParams}


Definition at line 1847 of file m\+\_\+mpi\+\_\+proxy.\+f90.

\mbox{\Hypertarget{namespacem__mpi__proxy_ac15a15ba12e6110bb015af0ece5bba47}\label{namespacem__mpi__proxy_ac15a15ba12e6110bb015af0ece5bba47}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!s\+\_\+mpi\+\_\+defragment\+\_\+1d\+\_\+grid\+\_\+variable@{s\+\_\+mpi\+\_\+defragment\+\_\+1d\+\_\+grid\+\_\+variable}}
\index{s\+\_\+mpi\+\_\+defragment\+\_\+1d\+\_\+grid\+\_\+variable@{s\+\_\+mpi\+\_\+defragment\+\_\+1d\+\_\+grid\+\_\+variable}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{s\+\_\+mpi\+\_\+defragment\+\_\+1d\+\_\+grid\+\_\+variable()}{s\_mpi\_defragment\_1d\_grid\_variable()}}
{\footnotesize\ttfamily subroutine m\+\_\+mpi\+\_\+proxy\+::s\+\_\+mpi\+\_\+defragment\+\_\+1d\+\_\+grid\+\_\+variable (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



This subroutine collects the sub-\/domain cell-\/boundary or cell-\/center locations data from all of the processors and puts back together the grid of the entire computational domain on the rank 0 processor. This is only done for 1D simulations. 



Definition at line 1781 of file m\+\_\+mpi\+\_\+proxy.\+f90.

\mbox{\Hypertarget{namespacem__mpi__proxy_a43fbda10c02ec8bc1fc572c83090f2e5}\label{namespacem__mpi__proxy_a43fbda10c02ec8bc1fc572c83090f2e5}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!s\+\_\+mpi\+\_\+finalize@{s\+\_\+mpi\+\_\+finalize}}
\index{s\+\_\+mpi\+\_\+finalize@{s\+\_\+mpi\+\_\+finalize}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{s\+\_\+mpi\+\_\+finalize()}{s\_mpi\_finalize()}}
{\footnotesize\ttfamily subroutine m\+\_\+mpi\+\_\+proxy\+::s\+\_\+mpi\+\_\+finalize (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Finalization of all M\+PI related processes. 



Definition at line 1894 of file m\+\_\+mpi\+\_\+proxy.\+f90.

\mbox{\Hypertarget{namespacem__mpi__proxy_a0b226acd5a9097566685604292c3ae0d}\label{namespacem__mpi__proxy_a0b226acd5a9097566685604292c3ae0d}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!s\+\_\+mpi\+\_\+gather\+\_\+data\+\_\+extents@{s\+\_\+mpi\+\_\+gather\+\_\+data\+\_\+extents}}
\index{s\+\_\+mpi\+\_\+gather\+\_\+data\+\_\+extents@{s\+\_\+mpi\+\_\+gather\+\_\+data\+\_\+extents}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{s\+\_\+mpi\+\_\+gather\+\_\+data\+\_\+extents()}{s\_mpi\_gather\_data\_extents()}}
{\footnotesize\ttfamily subroutine m\+\_\+mpi\+\_\+proxy\+::s\+\_\+mpi\+\_\+gather\+\_\+data\+\_\+extents (\begin{DoxyParamCaption}\item[{real(kind(0d0)), dimension(\+:,\+:,\+:), intent(in)}]{q\+\_\+sf,  }\item[{real(kind(0d0)), dimension(1\+:2,0\+:num\+\_\+procs-\/1), intent(inout)}]{data\+\_\+extents }\end{DoxyParamCaption})}



This subroutine gathers the Silo database metadata for the flow variable\textquotesingle{}s extents as to boost performance of the multidimensional visualization. 


\begin{DoxyParams}{Parameters}
{\em q\+\_\+sf} & Flow variable defined on a single computational sub-\/domain \\
\hline
{\em data\+\_\+extents} & The flow variable extents on each of the processor\textquotesingle{}s sub-\/domain. First dimension of array corresponds to the former\textquotesingle{}s minimum and maximum values, respectively, while second dimension corresponds to each processor\textquotesingle{}s rank. \\
\hline
\end{DoxyParams}


Definition at line 1816 of file m\+\_\+mpi\+\_\+proxy.\+f90.

\mbox{\Hypertarget{namespacem__mpi__proxy_ab2ced8f095b812fcc355539c2c5fa162}\label{namespacem__mpi__proxy_ab2ced8f095b812fcc355539c2c5fa162}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!s\+\_\+mpi\+\_\+gather\+\_\+spatial\+\_\+extents@{s\+\_\+mpi\+\_\+gather\+\_\+spatial\+\_\+extents}}
\index{s\+\_\+mpi\+\_\+gather\+\_\+spatial\+\_\+extents@{s\+\_\+mpi\+\_\+gather\+\_\+spatial\+\_\+extents}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{s\+\_\+mpi\+\_\+gather\+\_\+spatial\+\_\+extents()}{s\_mpi\_gather\_spatial\_extents()}}
{\footnotesize\ttfamily subroutine m\+\_\+mpi\+\_\+proxy\+::s\+\_\+mpi\+\_\+gather\+\_\+spatial\+\_\+extents (\begin{DoxyParamCaption}\item[{real(kind(0d0)), dimension(1\+:,0\+:), intent(inout)}]{spatial\+\_\+extents }\end{DoxyParamCaption})}



This subroutine gathers the Silo database metadata for the spatial extents in order to boost the performance of the multidimensional visualization. 


\begin{DoxyParams}{Parameters}
{\em spatial\+\_\+extents} & Spatial extents for each processor\textquotesingle{}s sub-\/domain. First dimension corresponds to the minimum and maximum values, respectively, while the second dimension corresponds to the processor rank. \\
\hline
\end{DoxyParams}


Definition at line 1661 of file m\+\_\+mpi\+\_\+proxy.\+f90.

\mbox{\Hypertarget{namespacem__mpi__proxy_a9bc4c617505152d3cc553e5bc25c1ee1}\label{namespacem__mpi__proxy_a9bc4c617505152d3cc553e5bc25c1ee1}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!s\+\_\+mpi\+\_\+initialize@{s\+\_\+mpi\+\_\+initialize}}
\index{s\+\_\+mpi\+\_\+initialize@{s\+\_\+mpi\+\_\+initialize}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{s\+\_\+mpi\+\_\+initialize()}{s\_mpi\_initialize()}}
{\footnotesize\ttfamily subroutine m\+\_\+mpi\+\_\+proxy\+::s\+\_\+mpi\+\_\+initialize (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



The subroutine intializes the M\+PI environment and queries both the number of processors that will be available for the job as well as the local processor rank. 



Definition at line 84 of file m\+\_\+mpi\+\_\+proxy.\+f90.

\mbox{\Hypertarget{namespacem__mpi__proxy_adaa028bb99f844487de8d6e4507e7734}\label{namespacem__mpi__proxy_adaa028bb99f844487de8d6e4507e7734}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!s\+\_\+mpi\+\_\+reduce\+\_\+maxloc@{s\+\_\+mpi\+\_\+reduce\+\_\+maxloc}}
\index{s\+\_\+mpi\+\_\+reduce\+\_\+maxloc@{s\+\_\+mpi\+\_\+reduce\+\_\+maxloc}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{s\+\_\+mpi\+\_\+reduce\+\_\+maxloc()}{s\_mpi\_reduce\_maxloc()}}
{\footnotesize\ttfamily subroutine m\+\_\+mpi\+\_\+proxy\+::s\+\_\+mpi\+\_\+reduce\+\_\+maxloc (\begin{DoxyParamCaption}\item[{real(kind(0d0)), dimension(2), intent(inout)}]{var\+\_\+loc }\end{DoxyParamCaption})}



The following subroutine takes the first element of the 2-\/element inputted variable and determines its maximum value on the entire computational domain. The result is stored back into the first element of the variable while the rank of the processor that is in charge of the sub-\/ domain containing the maximum is stored into the second element of the variable. 


\begin{DoxyParams}{Parameters}
{\em var\+\_\+loc} & On input, this variable holds the local value and processor rank, which are to be reduced among all the processors in communicator. On output, this variable holds the maximum value, reduced amongst all of the local values, and the process rank to which the value belongs. \\
\hline
\end{DoxyParams}


Definition at line 1631 of file m\+\_\+mpi\+\_\+proxy.\+f90.

\mbox{\Hypertarget{namespacem__mpi__proxy_a4d7ec0d1976967504babdf44ec83c1b1}\label{namespacem__mpi__proxy_a4d7ec0d1976967504babdf44ec83c1b1}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!s\+\_\+mpi\+\_\+sendrecv\+\_\+cons\+\_\+vars\+\_\+buffer\+\_\+regions@{s\+\_\+mpi\+\_\+sendrecv\+\_\+cons\+\_\+vars\+\_\+buffer\+\_\+regions}}
\index{s\+\_\+mpi\+\_\+sendrecv\+\_\+cons\+\_\+vars\+\_\+buffer\+\_\+regions@{s\+\_\+mpi\+\_\+sendrecv\+\_\+cons\+\_\+vars\+\_\+buffer\+\_\+regions}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{s\+\_\+mpi\+\_\+sendrecv\+\_\+cons\+\_\+vars\+\_\+buffer\+\_\+regions()}{s\_mpi\_sendrecv\_cons\_vars\_buffer\_regions()}}
{\footnotesize\ttfamily subroutine m\+\_\+mpi\+\_\+proxy\+::s\+\_\+mpi\+\_\+sendrecv\+\_\+cons\+\_\+vars\+\_\+buffer\+\_\+regions (\begin{DoxyParamCaption}\item[{type(\hyperlink{structm__derived__types_1_1scalar__field}{scalar\+\_\+field}), dimension(sys\+\_\+size), intent(inout)}]{q\+\_\+cons\+\_\+vf,  }\item[{character(len = 3), intent(in)}]{pbc\+\_\+loc,  }\item[{character, intent(in)}]{sweep\+\_\+coord }\end{DoxyParamCaption})}



Communicates buffer regions associated with conservative variables with processors in charge of the neighbooring sub-\/domains. 


\begin{DoxyParams}{Parameters}
{\em q\+\_\+cons\+\_\+vf} & Conservative variables \\
\hline
{\em pbc\+\_\+loc} & Processor boundary condition (P\+BC) location \\
\hline
{\em sweep\+\_\+coord} & Coordinate direction normal to the processor boundary \\
\hline
\end{DoxyParams}


Definition at line 1094 of file m\+\_\+mpi\+\_\+proxy.\+f90.

\mbox{\Hypertarget{namespacem__mpi__proxy_a8e48d59a04981a6594f25ad7a7562492}\label{namespacem__mpi__proxy_a8e48d59a04981a6594f25ad7a7562492}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!s\+\_\+mpi\+\_\+sendrecv\+\_\+grid\+\_\+vars\+\_\+buffer\+\_\+regions@{s\+\_\+mpi\+\_\+sendrecv\+\_\+grid\+\_\+vars\+\_\+buffer\+\_\+regions}}
\index{s\+\_\+mpi\+\_\+sendrecv\+\_\+grid\+\_\+vars\+\_\+buffer\+\_\+regions@{s\+\_\+mpi\+\_\+sendrecv\+\_\+grid\+\_\+vars\+\_\+buffer\+\_\+regions}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{s\+\_\+mpi\+\_\+sendrecv\+\_\+grid\+\_\+vars\+\_\+buffer\+\_\+regions()}{s\_mpi\_sendrecv\_grid\_vars\_buffer\_regions()}}
{\footnotesize\ttfamily subroutine m\+\_\+mpi\+\_\+proxy\+::s\+\_\+mpi\+\_\+sendrecv\+\_\+grid\+\_\+vars\+\_\+buffer\+\_\+regions (\begin{DoxyParamCaption}\item[{character(len = 3), intent(in)}]{pbc\+\_\+loc,  }\item[{character, intent(in)}]{sweep\+\_\+coord }\end{DoxyParamCaption})}



Communicates the buffer regions associated with the grid variables with processors in charge of the neighbooring sub-\/domains. Note that only cell-\/width spacings feature buffer regions so that no information relating to the cell-\/boundary locations is communicated. 


\begin{DoxyParams}{Parameters}
{\em pbc\+\_\+loc} & Processor boundary condition (P\+BC) location \\
\hline
{\em sweep\+\_\+coord} & Coordinate direction normal to the processor boundary \\
\hline
\end{DoxyParams}


Definition at line 892 of file m\+\_\+mpi\+\_\+proxy.\+f90.



\subsection{Variable Documentation}
\mbox{\Hypertarget{namespacem__mpi__proxy_aebaa6e3cc66d2431c5fb49896d40d7e6}\label{namespacem__mpi__proxy_aebaa6e3cc66d2431c5fb49896d40d7e6}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!displs@{displs}}
\index{displs@{displs}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{displs}{displs}}
{\footnotesize\ttfamily integer, dimension(\+:), allocatable m\+\_\+mpi\+\_\+proxy\+::displs}



Definition at line 67 of file m\+\_\+mpi\+\_\+proxy.\+f90.

\mbox{\Hypertarget{namespacem__mpi__proxy_ae5709407e3600d19d79b183e409bb982}\label{namespacem__mpi__proxy_ae5709407e3600d19d79b183e409bb982}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!err\+\_\+code@{err\+\_\+code}}
\index{err\+\_\+code@{err\+\_\+code}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{err\+\_\+code}{err\_code}}
{\footnotesize\ttfamily integer, private m\+\_\+mpi\+\_\+proxy\+::err\+\_\+code\hspace{0.3cm}{\ttfamily [private]}}



Definition at line 72 of file m\+\_\+mpi\+\_\+proxy.\+f90.

\mbox{\Hypertarget{namespacem__mpi__proxy_a306ba163b09cfc692125f2c0ba82ef8c}\label{namespacem__mpi__proxy_a306ba163b09cfc692125f2c0ba82ef8c}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!ierr@{ierr}}
\index{ierr@{ierr}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{ierr}{ierr}}
{\footnotesize\ttfamily integer, private m\+\_\+mpi\+\_\+proxy\+::ierr\hspace{0.3cm}{\ttfamily [private]}}



Definition at line 72 of file m\+\_\+mpi\+\_\+proxy.\+f90.

\mbox{\Hypertarget{namespacem__mpi__proxy_af08f3246d9efac15d6391dc4205d4611}\label{namespacem__mpi__proxy_af08f3246d9efac15d6391dc4205d4611}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!q\+\_\+cons\+\_\+buffer\+\_\+in@{q\+\_\+cons\+\_\+buffer\+\_\+in}}
\index{q\+\_\+cons\+\_\+buffer\+\_\+in@{q\+\_\+cons\+\_\+buffer\+\_\+in}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{q\+\_\+cons\+\_\+buffer\+\_\+in}{q\_cons\_buffer\_in}}
{\footnotesize\ttfamily real(kind(0d0)), dimension(\+:), allocatable m\+\_\+mpi\+\_\+proxy\+::q\+\_\+cons\+\_\+buffer\+\_\+in}



Definition at line 58 of file m\+\_\+mpi\+\_\+proxy.\+f90.

\mbox{\Hypertarget{namespacem__mpi__proxy_a3d449655a88a9e5248af35a82caf877a}\label{namespacem__mpi__proxy_a3d449655a88a9e5248af35a82caf877a}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!q\+\_\+cons\+\_\+buffer\+\_\+out@{q\+\_\+cons\+\_\+buffer\+\_\+out}}
\index{q\+\_\+cons\+\_\+buffer\+\_\+out@{q\+\_\+cons\+\_\+buffer\+\_\+out}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{q\+\_\+cons\+\_\+buffer\+\_\+out}{q\_cons\_buffer\_out}}
{\footnotesize\ttfamily real(kind(0d0)), dimension(\+:), allocatable m\+\_\+mpi\+\_\+proxy\+::q\+\_\+cons\+\_\+buffer\+\_\+out}



Definition at line 59 of file m\+\_\+mpi\+\_\+proxy.\+f90.

\mbox{\Hypertarget{namespacem__mpi__proxy_a2198e825f0884d4ee9e96b6efdb69cee}\label{namespacem__mpi__proxy_a2198e825f0884d4ee9e96b6efdb69cee}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!recvcounts@{recvcounts}}
\index{recvcounts@{recvcounts}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{recvcounts}{recvcounts}}
{\footnotesize\ttfamily integer, dimension(\+:), allocatable m\+\_\+mpi\+\_\+proxy\+::recvcounts}



Definition at line 66 of file m\+\_\+mpi\+\_\+proxy.\+f90.

