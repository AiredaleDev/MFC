\hypertarget{namespacem__mpi__proxy}{}\section{m\+\_\+mpi\+\_\+proxy Module Reference}
\label{namespacem__mpi__proxy}\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}


The module serves as a proxy to the parameters and subroutines available in the M\+PI implementation\textquotesingle{}s M\+PI module. Specifically, the purpose of the proxy is to harness basic M\+PI commands into more complicated procedures as to accomplish the communication goals for the simulation.  


\subsection*{Functions/\+Subroutines}
\begin{DoxyCompactItemize}
\item 
subroutine \hyperlink{namespacem__mpi__proxy_a9bc4c617505152d3cc553e5bc25c1ee1}{s\+\_\+mpi\+\_\+initialize} ()
\begin{DoxyCompactList}\small\item\em The subroutine intializes the M\+PI execution environment and queries both the number of processors which will be available for the job and the local processor rank. \end{DoxyCompactList}\item 
subroutine \hyperlink{namespacem__mpi__proxy_a04ac565bad2b22dc045a5eeb4f516e2e}{s\+\_\+mpi\+\_\+abort} ()
\begin{DoxyCompactList}\small\item\em The subroutine terminates the M\+PI execution environment. \end{DoxyCompactList}\item 
subroutine \hyperlink{namespacem__mpi__proxy_a2ff35ede51e90c483969e44c31303415}{s\+\_\+initialize\+\_\+mpi\+\_\+data} (q\+\_\+cons\+\_\+vf)
\begin{DoxyCompactList}\small\item\em The subroutine that initializes M\+PI data structures. \end{DoxyCompactList}\item 
subroutine \hyperlink{namespacem__mpi__proxy_abfbc42cea69273bc9fa4a2d78f636eb1}{s\+\_\+mpi\+\_\+barrier} ()
\begin{DoxyCompactList}\small\item\em Halts all processes until all have reached barrier. \end{DoxyCompactList}\item 
subroutine \hyperlink{namespacem__mpi__proxy_a015ee2c0892e9cfcb858da8f27b646d5}{s\+\_\+initialize\+\_\+mpi\+\_\+proxy\+\_\+module} ()
\begin{DoxyCompactList}\small\item\em The computation of parameters, the allocation of memory, the association of pointers and/or the execution of any other procedures that are necessary to setup the module. \end{DoxyCompactList}\item 
subroutine \hyperlink{namespacem__mpi__proxy_a69660c5fe9302a8c0496b622fa3b5286}{s\+\_\+mpi\+\_\+bcast\+\_\+user\+\_\+inputs} ()
\begin{DoxyCompactList}\small\item\em Since only the processor with rank 0 reads and verifies the consistency of user inputs, these are initially not available to the other processors. Then, the purpose of this subroutine is to distribute the user inputs to the remaining processors in the communicator. \end{DoxyCompactList}\item 
subroutine \hyperlink{namespacem__mpi__proxy_a80c5e235786545276fe6ffa06965017f}{s\+\_\+mpi\+\_\+decompose\+\_\+computational\+\_\+domain} ()
\begin{DoxyCompactList}\small\item\em The purpose of this procedure is to optimally decompose the computational domain among the available processors. This is performed by attempting to award each processor, in each of the coordinate directions, approximately the same number of cells, and then recomputing the affected global parameters. \end{DoxyCompactList}\item 
subroutine \hyperlink{namespacem__mpi__proxy_ac348cb6f02f2a9ab70d5c7eac6320231}{s\+\_\+mpi\+\_\+sendrecv\+\_\+grid\+\_\+variables\+\_\+buffers} (mpi\+\_\+dir, pbc\+\_\+loc)
\begin{DoxyCompactList}\small\item\em The goal of this procedure is to populate the buffers of the grid variables by communicating with the neighboring processors. Note that only the buffers of the cell-\/width distributions are handled in such a way. This is because the buffers of cell-\/boundary locations may be calculated directly from those of the cell-\/width distributions. \end{DoxyCompactList}\item 
subroutine \hyperlink{namespacem__mpi__proxy_a340a2c9b1688582ee485dd5d9bbcc864}{s\+\_\+mpi\+\_\+reduce\+\_\+stability\+\_\+criteria\+\_\+extrema} (icfl\+\_\+max\+\_\+loc, vcfl\+\_\+max\+\_\+loc, ccfl\+\_\+max\+\_\+loc, Rc\+\_\+min\+\_\+loc, icfl\+\_\+max\+\_\+glb, vcfl\+\_\+max\+\_\+glb, ccfl\+\_\+max\+\_\+glb, Rc\+\_\+min\+\_\+glb)
\begin{DoxyCompactList}\small\item\em The goal of this subroutine is to determine the global extrema of the stability criteria in the computational domain. This is performed by sifting through the local extrema of each stability criterion. Note that each of the local extrema is from a single process, within its assigned section of the computational domain. Finally, note that the global extrema values are only bookkeept on the rank 0 processor. \end{DoxyCompactList}\item 
subroutine \hyperlink{namespacem__mpi__proxy_a873520062481f3dabcfa77f9bd7728d3}{s\+\_\+mpi\+\_\+allreduce\+\_\+sum} (var\+\_\+loc, var\+\_\+glb)
\begin{DoxyCompactList}\small\item\em The following subroutine takes the input local variable from all processors and reduces to the sum of all values. The reduced variable is recorded back onto the original local variable on each processor. \end{DoxyCompactList}\item 
subroutine \hyperlink{namespacem__mpi__proxy_a398a372625502c0430a64e0bb23e2342}{s\+\_\+mpi\+\_\+allreduce\+\_\+min} (var\+\_\+loc, var\+\_\+glb)
\begin{DoxyCompactList}\small\item\em The following subroutine takes the input local variable from all processors and reduces to the minimum of all values. The reduced variable is recorded back onto the original local variable on each processor. \end{DoxyCompactList}\item 
subroutine \hyperlink{namespacem__mpi__proxy_aff7286de058c49b2a5d7a84be1983e8f}{s\+\_\+mpi\+\_\+allreduce\+\_\+max} (var\+\_\+loc, var\+\_\+glb)
\begin{DoxyCompactList}\small\item\em The following subroutine takes the input local variable from all processors and reduces to the maximum of all values. The reduced variable is recorded back onto the original local variable on each processor. \end{DoxyCompactList}\item 
subroutine \hyperlink{namespacem__mpi__proxy_a9015d32f3d27c21a75e8a916e5090b96}{s\+\_\+mpi\+\_\+sendrecv\+\_\+conservative\+\_\+variables\+\_\+buffers} (q\+\_\+cons\+\_\+vf, mpi\+\_\+dir, pbc\+\_\+loc)
\begin{DoxyCompactList}\small\item\em The goal of this procedure is to populate the buffers of the cell-\/average conservative variables by communicating with the neighboring processors. \end{DoxyCompactList}\item 
subroutine \hyperlink{namespacem__mpi__proxy_ac984c84fe4140876d6600250af9807da}{s\+\_\+finalize\+\_\+mpi\+\_\+proxy\+\_\+module} ()
\begin{DoxyCompactList}\small\item\em Module deallocation and/or disassociation procedures. \end{DoxyCompactList}\item 
subroutine \hyperlink{namespacem__mpi__proxy_a43fbda10c02ec8bc1fc572c83090f2e5}{s\+\_\+mpi\+\_\+finalize} ()
\begin{DoxyCompactList}\small\item\em The subroutine finalizes the M\+PI execution environment. \end{DoxyCompactList}\end{DoxyCompactItemize}
\subsection*{Variables}
\begin{DoxyCompactItemize}
\item 
real(kind(0d0)), dimension(\+:), allocatable, private \hyperlink{namespacem__mpi__proxy_a67f9b5b69858f72c61b8d8ebf212567d}{q\+\_\+cons\+\_\+buff\+\_\+send}
\begin{DoxyCompactList}\small\item\em This variable is utilized to pack and send the buffer of the cell-\/average conservative variables, for a single computational domain boundary at the time, to the relevant neighboring processor. \end{DoxyCompactList}\item 
real(kind(0d0)), dimension(\+:), allocatable, private \hyperlink{namespacem__mpi__proxy_a2915932883654eecbac1283ce03be3ba}{q\+\_\+cons\+\_\+buff\+\_\+recv}
\begin{DoxyCompactList}\small\item\em q\+\_\+cons\+\_\+buff\+\_\+recv is utilized to receive and unpack the buffer of the cell-\/ average conservative variables, for a single computational domain boundary at the time, from the relevant neighboring processor. \end{DoxyCompactList}\end{DoxyCompactItemize}
\begin{Indent}\textbf{ Generic flags used to identify and report M\+PI errors}\par
\begin{DoxyCompactItemize}
\item 
integer, private \hyperlink{namespacem__mpi__proxy_ae5709407e3600d19d79b183e409bb982}{err\+\_\+code}
\item 
integer, private \hyperlink{namespacem__mpi__proxy_a306ba163b09cfc692125f2c0ba82ef8c}{ierr}
\end{DoxyCompactItemize}
\end{Indent}


\subsection{Detailed Description}
The module serves as a proxy to the parameters and subroutines available in the M\+PI implementation\textquotesingle{}s M\+PI module. Specifically, the purpose of the proxy is to harness basic M\+PI commands into more complicated procedures as to accomplish the communication goals for the simulation. 

\subsection{Function/\+Subroutine Documentation}
\mbox{\Hypertarget{namespacem__mpi__proxy_ac984c84fe4140876d6600250af9807da}\label{namespacem__mpi__proxy_ac984c84fe4140876d6600250af9807da}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!s\+\_\+finalize\+\_\+mpi\+\_\+proxy\+\_\+module@{s\+\_\+finalize\+\_\+mpi\+\_\+proxy\+\_\+module}}
\index{s\+\_\+finalize\+\_\+mpi\+\_\+proxy\+\_\+module@{s\+\_\+finalize\+\_\+mpi\+\_\+proxy\+\_\+module}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{s\+\_\+finalize\+\_\+mpi\+\_\+proxy\+\_\+module()}{s\_finalize\_mpi\_proxy\_module()}}
{\footnotesize\ttfamily subroutine m\+\_\+mpi\+\_\+proxy\+::s\+\_\+finalize\+\_\+mpi\+\_\+proxy\+\_\+module (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Module deallocation and/or disassociation procedures. 



Definition at line 1650 of file m\+\_\+mpi\+\_\+proxy.\+f90.

\mbox{\Hypertarget{namespacem__mpi__proxy_a2ff35ede51e90c483969e44c31303415}\label{namespacem__mpi__proxy_a2ff35ede51e90c483969e44c31303415}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!s\+\_\+initialize\+\_\+mpi\+\_\+data@{s\+\_\+initialize\+\_\+mpi\+\_\+data}}
\index{s\+\_\+initialize\+\_\+mpi\+\_\+data@{s\+\_\+initialize\+\_\+mpi\+\_\+data}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{s\+\_\+initialize\+\_\+mpi\+\_\+data()}{s\_initialize\_mpi\_data()}}
{\footnotesize\ttfamily subroutine m\+\_\+mpi\+\_\+proxy\+::s\+\_\+initialize\+\_\+mpi\+\_\+data (\begin{DoxyParamCaption}\item[{type(\hyperlink{structm__derived__types_1_1scalar__field}{scalar\+\_\+field}), dimension(sys\+\_\+size), intent(in)}]{q\+\_\+cons\+\_\+vf }\end{DoxyParamCaption})}



The subroutine that initializes M\+PI data structures. 


\begin{DoxyParams}{Parameters}
{\em q\+\_\+cons\+\_\+vf} & Conservative variables \\
\hline
\end{DoxyParams}


Definition at line 116 of file m\+\_\+mpi\+\_\+proxy.\+f90.

\mbox{\Hypertarget{namespacem__mpi__proxy_a015ee2c0892e9cfcb858da8f27b646d5}\label{namespacem__mpi__proxy_a015ee2c0892e9cfcb858da8f27b646d5}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!s\+\_\+initialize\+\_\+mpi\+\_\+proxy\+\_\+module@{s\+\_\+initialize\+\_\+mpi\+\_\+proxy\+\_\+module}}
\index{s\+\_\+initialize\+\_\+mpi\+\_\+proxy\+\_\+module@{s\+\_\+initialize\+\_\+mpi\+\_\+proxy\+\_\+module}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{s\+\_\+initialize\+\_\+mpi\+\_\+proxy\+\_\+module()}{s\_initialize\_mpi\_proxy\_module()}}
{\footnotesize\ttfamily subroutine m\+\_\+mpi\+\_\+proxy\+::s\+\_\+initialize\+\_\+mpi\+\_\+proxy\+\_\+module (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



The computation of parameters, the allocation of memory, the association of pointers and/or the execution of any other procedures that are necessary to setup the module. 



Definition at line 166 of file m\+\_\+mpi\+\_\+proxy.\+f90.

\mbox{\Hypertarget{namespacem__mpi__proxy_a04ac565bad2b22dc045a5eeb4f516e2e}\label{namespacem__mpi__proxy_a04ac565bad2b22dc045a5eeb4f516e2e}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!s\+\_\+mpi\+\_\+abort@{s\+\_\+mpi\+\_\+abort}}
\index{s\+\_\+mpi\+\_\+abort@{s\+\_\+mpi\+\_\+abort}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{s\+\_\+mpi\+\_\+abort()}{s\_mpi\_abort()}}
{\footnotesize\ttfamily subroutine m\+\_\+mpi\+\_\+proxy\+::s\+\_\+mpi\+\_\+abort (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



The subroutine terminates the M\+PI execution environment. 



Definition at line 104 of file m\+\_\+mpi\+\_\+proxy.\+f90.

\mbox{\Hypertarget{namespacem__mpi__proxy_aff7286de058c49b2a5d7a84be1983e8f}\label{namespacem__mpi__proxy_aff7286de058c49b2a5d7a84be1983e8f}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!s\+\_\+mpi\+\_\+allreduce\+\_\+max@{s\+\_\+mpi\+\_\+allreduce\+\_\+max}}
\index{s\+\_\+mpi\+\_\+allreduce\+\_\+max@{s\+\_\+mpi\+\_\+allreduce\+\_\+max}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{s\+\_\+mpi\+\_\+allreduce\+\_\+max()}{s\_mpi\_allreduce\_max()}}
{\footnotesize\ttfamily subroutine m\+\_\+mpi\+\_\+proxy\+::s\+\_\+mpi\+\_\+allreduce\+\_\+max (\begin{DoxyParamCaption}\item[{real(kind(0d0)), intent(in)}]{var\+\_\+loc,  }\item[{real(kind(0d0)), intent(out)}]{var\+\_\+glb }\end{DoxyParamCaption})}



The following subroutine takes the input local variable from all processors and reduces to the maximum of all values. The reduced variable is recorded back onto the original local variable on each processor. 


\begin{DoxyParams}{Parameters}
{\em var\+\_\+loc} & Some variable containing the local value which should be reduced amongst all the processors in the communicator. \\
\hline
{\em var\+\_\+glb} & The globally reduced value \\
\hline
\end{DoxyParams}


Definition at line 1162 of file m\+\_\+mpi\+\_\+proxy.\+f90.

\mbox{\Hypertarget{namespacem__mpi__proxy_a398a372625502c0430a64e0bb23e2342}\label{namespacem__mpi__proxy_a398a372625502c0430a64e0bb23e2342}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!s\+\_\+mpi\+\_\+allreduce\+\_\+min@{s\+\_\+mpi\+\_\+allreduce\+\_\+min}}
\index{s\+\_\+mpi\+\_\+allreduce\+\_\+min@{s\+\_\+mpi\+\_\+allreduce\+\_\+min}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{s\+\_\+mpi\+\_\+allreduce\+\_\+min()}{s\_mpi\_allreduce\_min()}}
{\footnotesize\ttfamily subroutine m\+\_\+mpi\+\_\+proxy\+::s\+\_\+mpi\+\_\+allreduce\+\_\+min (\begin{DoxyParamCaption}\item[{real(kind(0d0)), intent(in)}]{var\+\_\+loc,  }\item[{real(kind(0d0)), intent(out)}]{var\+\_\+glb }\end{DoxyParamCaption})}



The following subroutine takes the input local variable from all processors and reduces to the minimum of all values. The reduced variable is recorded back onto the original local variable on each processor. 


\begin{DoxyParams}{Parameters}
{\em var\+\_\+loc} & Some variable containing the local value which should be reduced amongst all the processors in the communicator. \\
\hline
{\em var\+\_\+glb} & The globally reduced value \\
\hline
\end{DoxyParams}


Definition at line 1142 of file m\+\_\+mpi\+\_\+proxy.\+f90.

\mbox{\Hypertarget{namespacem__mpi__proxy_a873520062481f3dabcfa77f9bd7728d3}\label{namespacem__mpi__proxy_a873520062481f3dabcfa77f9bd7728d3}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!s\+\_\+mpi\+\_\+allreduce\+\_\+sum@{s\+\_\+mpi\+\_\+allreduce\+\_\+sum}}
\index{s\+\_\+mpi\+\_\+allreduce\+\_\+sum@{s\+\_\+mpi\+\_\+allreduce\+\_\+sum}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{s\+\_\+mpi\+\_\+allreduce\+\_\+sum()}{s\_mpi\_allreduce\_sum()}}
{\footnotesize\ttfamily subroutine m\+\_\+mpi\+\_\+proxy\+::s\+\_\+mpi\+\_\+allreduce\+\_\+sum (\begin{DoxyParamCaption}\item[{real(kind(0d0)), intent(in)}]{var\+\_\+loc,  }\item[{real(kind(0d0)), intent(out)}]{var\+\_\+glb }\end{DoxyParamCaption})}



The following subroutine takes the input local variable from all processors and reduces to the sum of all values. The reduced variable is recorded back onto the original local variable on each processor. 


\begin{DoxyParams}{Parameters}
{\em var\+\_\+loc} & Some variable containing the local value which should be reduced amongst all the processors in the communicator. \\
\hline
{\em var\+\_\+glb} & The globally reduced value \\
\hline
\end{DoxyParams}


Definition at line 1122 of file m\+\_\+mpi\+\_\+proxy.\+f90.

\mbox{\Hypertarget{namespacem__mpi__proxy_abfbc42cea69273bc9fa4a2d78f636eb1}\label{namespacem__mpi__proxy_abfbc42cea69273bc9fa4a2d78f636eb1}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!s\+\_\+mpi\+\_\+barrier@{s\+\_\+mpi\+\_\+barrier}}
\index{s\+\_\+mpi\+\_\+barrier@{s\+\_\+mpi\+\_\+barrier}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{s\+\_\+mpi\+\_\+barrier()}{s\_mpi\_barrier()}}
{\footnotesize\ttfamily subroutine m\+\_\+mpi\+\_\+proxy\+::s\+\_\+mpi\+\_\+barrier (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Halts all processes until all have reached barrier. 



Definition at line 154 of file m\+\_\+mpi\+\_\+proxy.\+f90.

\mbox{\Hypertarget{namespacem__mpi__proxy_a69660c5fe9302a8c0496b622fa3b5286}\label{namespacem__mpi__proxy_a69660c5fe9302a8c0496b622fa3b5286}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!s\+\_\+mpi\+\_\+bcast\+\_\+user\+\_\+inputs@{s\+\_\+mpi\+\_\+bcast\+\_\+user\+\_\+inputs}}
\index{s\+\_\+mpi\+\_\+bcast\+\_\+user\+\_\+inputs@{s\+\_\+mpi\+\_\+bcast\+\_\+user\+\_\+inputs}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{s\+\_\+mpi\+\_\+bcast\+\_\+user\+\_\+inputs()}{s\_mpi\_bcast\_user\_inputs()}}
{\footnotesize\ttfamily subroutine m\+\_\+mpi\+\_\+proxy\+::s\+\_\+mpi\+\_\+bcast\+\_\+user\+\_\+inputs (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Since only the processor with rank 0 reads and verifies the consistency of user inputs, these are initially not available to the other processors. Then, the purpose of this subroutine is to distribute the user inputs to the remaining processors in the communicator. 



Definition at line 202 of file m\+\_\+mpi\+\_\+proxy.\+f90.

\mbox{\Hypertarget{namespacem__mpi__proxy_a80c5e235786545276fe6ffa06965017f}\label{namespacem__mpi__proxy_a80c5e235786545276fe6ffa06965017f}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!s\+\_\+mpi\+\_\+decompose\+\_\+computational\+\_\+domain@{s\+\_\+mpi\+\_\+decompose\+\_\+computational\+\_\+domain}}
\index{s\+\_\+mpi\+\_\+decompose\+\_\+computational\+\_\+domain@{s\+\_\+mpi\+\_\+decompose\+\_\+computational\+\_\+domain}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{s\+\_\+mpi\+\_\+decompose\+\_\+computational\+\_\+domain()}{s\_mpi\_decompose\_computational\_domain()}}
{\footnotesize\ttfamily subroutine m\+\_\+mpi\+\_\+proxy\+::s\+\_\+mpi\+\_\+decompose\+\_\+computational\+\_\+domain (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



The purpose of this procedure is to optimally decompose the computational domain among the available processors. This is performed by attempting to award each processor, in each of the coordinate directions, approximately the same number of cells, and then recomputing the affected global parameters. 



Definition at line 492 of file m\+\_\+mpi\+\_\+proxy.\+f90.

\mbox{\Hypertarget{namespacem__mpi__proxy_a43fbda10c02ec8bc1fc572c83090f2e5}\label{namespacem__mpi__proxy_a43fbda10c02ec8bc1fc572c83090f2e5}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!s\+\_\+mpi\+\_\+finalize@{s\+\_\+mpi\+\_\+finalize}}
\index{s\+\_\+mpi\+\_\+finalize@{s\+\_\+mpi\+\_\+finalize}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{s\+\_\+mpi\+\_\+finalize()}{s\_mpi\_finalize()}}
{\footnotesize\ttfamily subroutine m\+\_\+mpi\+\_\+proxy\+::s\+\_\+mpi\+\_\+finalize (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



The subroutine finalizes the M\+PI execution environment. 



Definition at line 1662 of file m\+\_\+mpi\+\_\+proxy.\+f90.

\mbox{\Hypertarget{namespacem__mpi__proxy_a9bc4c617505152d3cc553e5bc25c1ee1}\label{namespacem__mpi__proxy_a9bc4c617505152d3cc553e5bc25c1ee1}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!s\+\_\+mpi\+\_\+initialize@{s\+\_\+mpi\+\_\+initialize}}
\index{s\+\_\+mpi\+\_\+initialize@{s\+\_\+mpi\+\_\+initialize}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{s\+\_\+mpi\+\_\+initialize()}{s\_mpi\_initialize()}}
{\footnotesize\ttfamily subroutine m\+\_\+mpi\+\_\+proxy\+::s\+\_\+mpi\+\_\+initialize (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



The subroutine intializes the M\+PI execution environment and queries both the number of processors which will be available for the job and the local processor rank. 



Definition at line 77 of file m\+\_\+mpi\+\_\+proxy.\+f90.

\mbox{\Hypertarget{namespacem__mpi__proxy_a340a2c9b1688582ee485dd5d9bbcc864}\label{namespacem__mpi__proxy_a340a2c9b1688582ee485dd5d9bbcc864}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!s\+\_\+mpi\+\_\+reduce\+\_\+stability\+\_\+criteria\+\_\+extrema@{s\+\_\+mpi\+\_\+reduce\+\_\+stability\+\_\+criteria\+\_\+extrema}}
\index{s\+\_\+mpi\+\_\+reduce\+\_\+stability\+\_\+criteria\+\_\+extrema@{s\+\_\+mpi\+\_\+reduce\+\_\+stability\+\_\+criteria\+\_\+extrema}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{s\+\_\+mpi\+\_\+reduce\+\_\+stability\+\_\+criteria\+\_\+extrema()}{s\_mpi\_reduce\_stability\_criteria\_extrema()}}
{\footnotesize\ttfamily subroutine m\+\_\+mpi\+\_\+proxy\+::s\+\_\+mpi\+\_\+reduce\+\_\+stability\+\_\+criteria\+\_\+extrema (\begin{DoxyParamCaption}\item[{real(kind(0d0)), intent(in)}]{icfl\+\_\+max\+\_\+loc,  }\item[{real(kind(0d0)), intent(in)}]{vcfl\+\_\+max\+\_\+loc,  }\item[{real(kind(0d0)), intent(in)}]{ccfl\+\_\+max\+\_\+loc,  }\item[{real(kind(0d0)), intent(in)}]{Rc\+\_\+min\+\_\+loc,  }\item[{real(kind(0d0)), intent(out)}]{icfl\+\_\+max\+\_\+glb,  }\item[{real(kind(0d0)), intent(out)}]{vcfl\+\_\+max\+\_\+glb,  }\item[{real(kind(0d0)), intent(out)}]{ccfl\+\_\+max\+\_\+glb,  }\item[{real(kind(0d0)), intent(out)}]{Rc\+\_\+min\+\_\+glb }\end{DoxyParamCaption})}



The goal of this subroutine is to determine the global extrema of the stability criteria in the computational domain. This is performed by sifting through the local extrema of each stability criterion. Note that each of the local extrema is from a single process, within its assigned section of the computational domain. Finally, note that the global extrema values are only bookkeept on the rank 0 processor. 


\begin{DoxyParams}{Parameters}
{\em icfl\+\_\+max\+\_\+loc} & Local maximum I\+C\+FL stability criterion \\
\hline
{\em vcfl\+\_\+max\+\_\+loc} & Local maximum V\+C\+FL stability criterion \\
\hline
{\em ccfl\+\_\+max\+\_\+loc} & Local maximum C\+C\+FL stability criterion \\
\hline
{\em Rc\+\_\+min\+\_\+loc} & Local minimum Rc stability criterion \\
\hline
{\em icfl\+\_\+max\+\_\+glb} & Global maximum I\+C\+FL stability criterion \\
\hline
{\em vcfl\+\_\+max\+\_\+glb} & Global maximum V\+C\+FL stability criterion \\
\hline
{\em ccfl\+\_\+max\+\_\+glb} & Global maximum C\+C\+FL stability criterion \\
\hline
{\em Rc\+\_\+min\+\_\+glb} & Global minimum Rc stability criterion \\
\hline
\end{DoxyParams}


Definition at line 1076 of file m\+\_\+mpi\+\_\+proxy.\+f90.

\mbox{\Hypertarget{namespacem__mpi__proxy_a9015d32f3d27c21a75e8a916e5090b96}\label{namespacem__mpi__proxy_a9015d32f3d27c21a75e8a916e5090b96}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!s\+\_\+mpi\+\_\+sendrecv\+\_\+conservative\+\_\+variables\+\_\+buffers@{s\+\_\+mpi\+\_\+sendrecv\+\_\+conservative\+\_\+variables\+\_\+buffers}}
\index{s\+\_\+mpi\+\_\+sendrecv\+\_\+conservative\+\_\+variables\+\_\+buffers@{s\+\_\+mpi\+\_\+sendrecv\+\_\+conservative\+\_\+variables\+\_\+buffers}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{s\+\_\+mpi\+\_\+sendrecv\+\_\+conservative\+\_\+variables\+\_\+buffers()}{s\_mpi\_sendrecv\_conservative\_variables\_buffers()}}
{\footnotesize\ttfamily subroutine m\+\_\+mpi\+\_\+proxy\+::s\+\_\+mpi\+\_\+sendrecv\+\_\+conservative\+\_\+variables\+\_\+buffers (\begin{DoxyParamCaption}\item[{type(\hyperlink{structm__derived__types_1_1scalar__field}{scalar\+\_\+field}), dimension(sys\+\_\+size), intent(inout)}]{q\+\_\+cons\+\_\+vf,  }\item[{integer, intent(in)}]{mpi\+\_\+dir,  }\item[{integer, intent(in)}]{pbc\+\_\+loc }\end{DoxyParamCaption})}



The goal of this procedure is to populate the buffers of the cell-\/average conservative variables by communicating with the neighboring processors. 


\begin{DoxyParams}{Parameters}
{\em q\+\_\+cons\+\_\+vf} & Cell-\/average conservative variables \\
\hline
{\em mpi\+\_\+dir} & M\+PI communication coordinate direction \\
\hline
{\em pbc\+\_\+loc} & Processor boundary condition (P\+BC) location \\
\hline
\end{DoxyParams}


Definition at line 1183 of file m\+\_\+mpi\+\_\+proxy.\+f90.

\mbox{\Hypertarget{namespacem__mpi__proxy_ac348cb6f02f2a9ab70d5c7eac6320231}\label{namespacem__mpi__proxy_ac348cb6f02f2a9ab70d5c7eac6320231}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!s\+\_\+mpi\+\_\+sendrecv\+\_\+grid\+\_\+variables\+\_\+buffers@{s\+\_\+mpi\+\_\+sendrecv\+\_\+grid\+\_\+variables\+\_\+buffers}}
\index{s\+\_\+mpi\+\_\+sendrecv\+\_\+grid\+\_\+variables\+\_\+buffers@{s\+\_\+mpi\+\_\+sendrecv\+\_\+grid\+\_\+variables\+\_\+buffers}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{s\+\_\+mpi\+\_\+sendrecv\+\_\+grid\+\_\+variables\+\_\+buffers()}{s\_mpi\_sendrecv\_grid\_variables\_buffers()}}
{\footnotesize\ttfamily subroutine m\+\_\+mpi\+\_\+proxy\+::s\+\_\+mpi\+\_\+sendrecv\+\_\+grid\+\_\+variables\+\_\+buffers (\begin{DoxyParamCaption}\item[{integer, intent(in)}]{mpi\+\_\+dir,  }\item[{integer, intent(in)}]{pbc\+\_\+loc }\end{DoxyParamCaption})}



The goal of this procedure is to populate the buffers of the grid variables by communicating with the neighboring processors. Note that only the buffers of the cell-\/width distributions are handled in such a way. This is because the buffers of cell-\/boundary locations may be calculated directly from those of the cell-\/width distributions. 


\begin{DoxyParams}{Parameters}
{\em mpi\+\_\+dir} & M\+PI communication coordinate direction \\
\hline
{\em pbc\+\_\+loc} & Processor boundary condition (P\+BC) location \\
\hline
\end{DoxyParams}


Definition at line 876 of file m\+\_\+mpi\+\_\+proxy.\+f90.



\subsection{Variable Documentation}
\mbox{\Hypertarget{namespacem__mpi__proxy_ae5709407e3600d19d79b183e409bb982}\label{namespacem__mpi__proxy_ae5709407e3600d19d79b183e409bb982}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!err\+\_\+code@{err\+\_\+code}}
\index{err\+\_\+code@{err\+\_\+code}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{err\+\_\+code}{err\_code}}
{\footnotesize\ttfamily integer, private m\+\_\+mpi\+\_\+proxy\+::err\+\_\+code\hspace{0.3cm}{\ttfamily [private]}}



Definition at line 66 of file m\+\_\+mpi\+\_\+proxy.\+f90.

\mbox{\Hypertarget{namespacem__mpi__proxy_a306ba163b09cfc692125f2c0ba82ef8c}\label{namespacem__mpi__proxy_a306ba163b09cfc692125f2c0ba82ef8c}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!ierr@{ierr}}
\index{ierr@{ierr}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{ierr}{ierr}}
{\footnotesize\ttfamily integer, private m\+\_\+mpi\+\_\+proxy\+::ierr\hspace{0.3cm}{\ttfamily [private]}}



Definition at line 66 of file m\+\_\+mpi\+\_\+proxy.\+f90.

\mbox{\Hypertarget{namespacem__mpi__proxy_a2915932883654eecbac1283ce03be3ba}\label{namespacem__mpi__proxy_a2915932883654eecbac1283ce03be3ba}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!q\+\_\+cons\+\_\+buff\+\_\+recv@{q\+\_\+cons\+\_\+buff\+\_\+recv}}
\index{q\+\_\+cons\+\_\+buff\+\_\+recv@{q\+\_\+cons\+\_\+buff\+\_\+recv}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{q\+\_\+cons\+\_\+buff\+\_\+recv}{q\_cons\_buff\_recv}}
{\footnotesize\ttfamily real(kind(0d0)), dimension(\+:), allocatable, private m\+\_\+mpi\+\_\+proxy\+::q\+\_\+cons\+\_\+buff\+\_\+recv\hspace{0.3cm}{\ttfamily [private]}}



q\+\_\+cons\+\_\+buff\+\_\+recv is utilized to receive and unpack the buffer of the cell-\/ average conservative variables, for a single computational domain boundary at the time, from the relevant neighboring processor. 



Definition at line 59 of file m\+\_\+mpi\+\_\+proxy.\+f90.

\mbox{\Hypertarget{namespacem__mpi__proxy_a67f9b5b69858f72c61b8d8ebf212567d}\label{namespacem__mpi__proxy_a67f9b5b69858f72c61b8d8ebf212567d}} 
\index{m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}!q\+\_\+cons\+\_\+buff\+\_\+send@{q\+\_\+cons\+\_\+buff\+\_\+send}}
\index{q\+\_\+cons\+\_\+buff\+\_\+send@{q\+\_\+cons\+\_\+buff\+\_\+send}!m\+\_\+mpi\+\_\+proxy@{m\+\_\+mpi\+\_\+proxy}}
\subsubsection{\texorpdfstring{q\+\_\+cons\+\_\+buff\+\_\+send}{q\_cons\_buff\_send}}
{\footnotesize\ttfamily real(kind(0d0)), dimension(\+:), allocatable, private m\+\_\+mpi\+\_\+proxy\+::q\+\_\+cons\+\_\+buff\+\_\+send\hspace{0.3cm}{\ttfamily [private]}}



This variable is utilized to pack and send the buffer of the cell-\/average conservative variables, for a single computational domain boundary at the time, to the relevant neighboring processor. 



Definition at line 54 of file m\+\_\+mpi\+\_\+proxy.\+f90.

